{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"gray\">DOING COMPUTATIONAL SOCIAL SCIENCE<br>MODULE 3 <strong>PROBLEM SETS</strong></font>\n",
    "\n",
    "# <font color=\"#49699E\" size=40>MODULE 3 </font>\n",
    "\n",
    "\n",
    "# What You Need to Know Before Getting Started\n",
    "\n",
    "- **Every notebook assignment has an accompanying quiz**. Your work in each notebook assignment will serve as the basis for your quiz answers.\n",
    "- **You can consult any resources you want when completing these exercises and problems**. Just as it is in the \"real world:\" if you can't figure out how to do something, look it up. My recommendation is that you check the relevant parts of the assigned reading or search for inspiration on [https://stackoverflow.com](https://stackoverflow.com).\n",
    "- **Each problem is worth 1 point**. All problems are equally weighted.\n",
    "- **The information you need for each problem set is provided in the blue and green cells.** General instructions / the problem set preamble are in the blue cells, and instructions for specific problems are in the green cells. **You have to execute all of the code in the problem set, but you are only responsible for entering code into the code cells that immediately follow a green cell**. You will also recognize those cells because they will be incomplete. You need to replace each blank `▰▰#▰▰` with the code that will make the cell execute properly (where # is a sequentially-increasing integer, one for each blank).\n",
    "- Most modules will contain at least one question that requires you to load data from disk; **it is up to you to locate the data, place it in an appropriate directory on your local machine, and replace any instances of the `PATH_TO_DATA` variable with a path to the directory containing the relevant data**.\n",
    "- **The comments in the problem cells contain clues indicating what the following line of code is supposed to do.** Use these comments as a guide when filling in the blanks. \n",
    "- **You can ask for help**. \n",
    "\n",
    "Finally, remember that you do not need to \"master\" this content before moving on to other course materials, as what is introduced here is reinforced throughout the rest of the course. You will have plenty of time to practice and cement your new knowledge and skills.\n",
    "<div class='alert alert-block alert-danger'>As you complete this assignment, you may encounter variables that can be assigned a wide variety of different names. Rather than forcing you to employ a particular convention, we leave the naming of these variables up to you. During the quiz, submit an answer of 'USER_DEFINED' (without the quotation marks) to fill in any blank that you assigned an arbitrary name to. In most circumstances, this will occur due to the presence of a local iterator in a for-loop.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "Wading into endlessly nested JSON structures and sorting through text that's full of HTML tags can be a bit daunting. Thankfully, the well-structured nature of API responses means that more straightforward numeric data is usually easily accessed. Before starting, make sure your `cred.py` is properly setup like the example in the chapter.\n",
    "</div>\n",
    "<div class='alert alert-block alert-success'>\n",
    "In this exercise, you will search the Guardian API for the term \"morrison\" between the dates of 2019-05-18 and 2019-05-19 - the day Australian Prime Minister Scott Morrison won an election and the day after. Using a loop, search for the term in each of the three production offices ('aus', 'uk', and 'us') and store each result in a list. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aus: 23\n",
      "uk: 1\n",
      "us: 0\n"
     ]
    }
   ],
   "source": [
    "import cred\n",
    "\n",
    "# API key provided by the Guardian\n",
    "GUARDIAN_KEY = cred.GUARDIAN_KEY\n",
    "\n",
    "# Initialize Constants:\n",
    "API_ENDPOINT = 'http://content.guardianapis.com/search' \n",
    "office_list = ['aus','uk','us']\n",
    "\n",
    "# Set up the request parameters, including authorization\n",
    "PARAMS = {'api-key': GUARDIAN_KEY,                \n",
    "             'from-date': '2019-05-18',\n",
    "             'to-date': '2019-05-19',\n",
    "             'q': 'morrison'}\n",
    "\n",
    "# Initialize list for storing results from iteration\n",
    "morrison_list = []\n",
    "\n",
    "# Iterate over each of the offices in 'office list'\n",
    "for office in office_list:\n",
    "    \n",
    "    PARAMS['production-office'] = office      # set the search filter on each pass of the loop\n",
    "    response = requests.get(API_ENDPOINT, params=PARAMS)    # send the query to the Guardian API\n",
    "    response_dict = response.json()['response']        # keep the relevant component of the response\n",
    "    \n",
    "    total_articles = response_dict['total']        # access the metafield that indicates the total number of responses\n",
    "    morrison_list.append(total_articles)           # add the resulting data to the list of results\n",
    "    \n",
    "    print(office + ': ' + str(total_articles))        # print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "Next we'll grab another search term - this time 'trump' - for the same time period and the same set of three production offices. \n",
    "</div>\n",
    "<div class='alert alert-block alert-success'>\n",
    "Query the Guardian API for 'trump' and store the results in a list much like last time. Create a pandas dataframe out of all three lists, giving each column a reasonably descriptive name. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>office</th>\n",
       "      <th>morrison</th>\n",
       "      <th>trump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aus</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uk</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  office  morrison  trump\n",
       "0    aus        23      6\n",
       "1     uk         1     20\n",
       "2     us         0     18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize list for storing results from iteration\n",
    "trump_list = []\n",
    "\n",
    "#Modify the search term\n",
    "PARAMS['q'] = 'trump'                  \n",
    "\n",
    "# Iterate over each of the offices in 'office list'\n",
    "for office in office_list:\n",
    "    \n",
    "    PARAMS['production-office'] = office # set the search filter on each pass of the loop\n",
    "    \n",
    "    response = requests.get(API_ENDPOINT, params=PARAMS)  # send the query to the Guardian API\n",
    "    response_dict = response.json()['response'] # keep the relevant component of the response\n",
    "    \n",
    "    total_articles = response_dict['total'] # access the metafield that indicates the total number of responses\n",
    "    trump_list.append(total_articles)       # add the resulting data to the list of results\n",
    "\n",
    "# Initialize an empty dataframe and create columns from the lists\n",
    "df = pd.DataFrame()\n",
    "df['office'], df['morrison'], df['trump'] = office_list, morrison_list, trump_list       \n",
    "\n",
    "# See the whole (small) dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "Let's take a look at what kind of stories from the UK office were mentioning Trump in that time period.\n",
    "</div>\n",
    "<div class='alert alert-block alert-success'>\n",
    "Reconfigure the appropriate `PARAMS` dictionary entries to carry out the search, adding 'headline' to the request. Retrieve the headlines from each article returned in the response, store them in a list, and take a look at the topics suggested by the headlines! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"BP pushed for Arctic drilling rights after Trump's election\",\n",
      " \"'Tariff': what's in the word behind Trump's tit-for-tat trade war?\",\n",
      " 'Jeremy Kyle Show: why take so long to end this daily humiliation?',\n",
      " 'Old grudges, new weapons… is the US on the brink of war with Iran?',\n",
      " 'Too much has been sacrificed to allow Brexit to destroy Europe’s unity',\n",
      " 'Footballer Héctor Bellerín calls on sport to oppose Alabama abortion ban',\n",
      " 'What my queer journey taught me about love',\n",
      " \"Julianna Margulies on her shocking Ebola drama: 'I panicked in my hazmat \"\n",
      " \"suit!'\",\n",
      " \"Theresa May prepares 'bold' last-ditch offer to MPs on Brexit bill\",\n",
      " 'Don’t lead us to disaster, moderate Tories warn frontrunner Boris Johnson']\n"
     ]
    }
   ],
   "source": [
    "# Initialize list for storing results from iteration\n",
    "trump_uk_headlines = []\n",
    "\n",
    "# Change some of the request parameters again\n",
    "PARAMS['q'] = 'trump'                          \n",
    "PARAMS['production-office'] = 'uk'\n",
    "PARAMS['show-fields'] = 'headline'\n",
    "\n",
    "\n",
    "response = requests.get(API_ENDPOINT, params=PARAMS)  # send the query to the Guardian API\n",
    "response_dict = response.json()['response']           # keep the relevant component of the response\n",
    "\n",
    "# Iterate over each of the responses\n",
    "for resp in response_dict['results']:\n",
    "    headline = resp['fields']['headline']        # process the new result field\n",
    "    trump_uk_headlines.append(headline)          # add the resulting data to the list of results\n",
    "    \n",
    "# View the list you just finished making\n",
    "pprint(trump_uk_headlines) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "Word counts are also often at least a byproduct of many API processes, so they will often be available. Let's see if there was much difference in the average word count of articles mentioning Trump, across all production offices, compared to those mentioning Morrison.\n",
    "</div>\n",
    "<div class='alert alert-block alert-success'>\n",
    "Modify the relevant entries in `PARAMS`, retrieving the wordcount for each article in the response text. Results should be stored in a list for each politician to calculate and print the average word counts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629.0\n",
      "804.9\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists for storing results from iteration\n",
    "morrison_counts = []\n",
    "trump_counts = []\n",
    "\n",
    "# Need to request a new field and clear the office filter\n",
    "PARAMS['q'] = 'morrison'\n",
    "PARAMS['production-office'] = None\n",
    "PARAMS['show-fields'] = 'wordcount'             \n",
    "\n",
    "response = requests.get(API_ENDPOINT, params=PARAMS)   # send the query to the Guardian API\n",
    "response_dict = response.json()['response']            # keep the relevant component of the response \n",
    "\n",
    "# Fill the list of words counts for the first search\n",
    "for resp in response_dict['results']:\n",
    "    wordcount = resp['fields']['wordcount']       # retrieve the word count\n",
    "    morrison_counts.append(int(wordcount))        # int() is needed because the API returns strings\n",
    "\n",
    "PARAMS['q'] = 'trump'\n",
    "response = requests.get(API_ENDPOINT, params=PARAMS)   # send the query to the Guardian API\n",
    "response_dict = response.json()['response']            # keep the relevant component of the response\n",
    "\n",
    "# Fill the list of words counts for the second search\n",
    "for resp in response_dict['results']:\n",
    "    wordcount = resp['fields']['wordcount']\n",
    "    trump_counts.append(int(wordcount))\n",
    "\n",
    "# Calculate the average of the word counts\n",
    "morrison_avg = np.mean(morrison_counts)        \n",
    "print(morrison_avg)\n",
    "trump_avg = np.mean(trump_counts)\n",
    "print(trump_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "For this exercise, we're going to use the power of scraping to delve into the results of the 2019 Canadian Federal Election! Just as every journey begins with a single step, we're going to start out with some basics. \n",
    "</div>\n",
    "<div class='alert alert-block alert-success'>\n",
    "Use the URL for Wikipedia's riding-by-riding election results to retrieve the web page and then check to see the if the result from the server was ok.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Store our website address in the 'url' variable\n",
    "url = \"https://en.wikipedia.org/wiki/Results_of_the_2019_Canadian_federal_election\"\n",
    "\n",
    "# Retrieve the website\n",
    "r = requests.get(url)\n",
    "\n",
    "# Query as to whether or not our request was 'ok' and display result\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "Now that we have the HTML in hand, let's use BeautifulSoup to process the website and get its on-screen title (the text that's immediately above the 'From Wikipedia, the free encyclopedia'). Since the on-screen title differs somewhat from the tab title that we retreived in the chapter on scraping, you might need to do a little digging in the site's HTML to figure out where it's stored. (Hint: you can find it in the 'body' of the article, and it is a type of heading)\n",
    "</div>\n",
    "<div class='alert alert-block alert-success'>\n",
    "Process the website using BeautifulSoup and find the on-screen title of the website we retrieved. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of the 2019 Canadian federal election\n"
     ]
    }
   ],
   "source": [
    "# Use BeautifulSoup to create an HTML DOM\n",
    "soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "# Use the soup object to find the text of the web page's on-screen title\n",
    "on_screen_title = soup.findAll(\"h1\")[0].text\n",
    "\n",
    "# Display Result\n",
    "print(on_screen_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "If you scroll around on the webpage we're scraping information from, you might notice that the centrepiece of the page is a large table entitled 'Results by riding - 2019 Canadian federal election'. It contains a whole lot of data; it would be great to have access to all of it in an organized fashion! Fortunately, we can easily find tables in this article by searching for objects with the 'table' tag. Unfortunately, there are 27 such tables in the article, and we can't be certain of the order they appear in BeautifulSoup's 'findAll' results. There are many ways to programmatically ensure that you've got the correct table. In this question, we're going to take advantage of the fact that we know the name of the table we're looking for. \n",
    "</div>\n",
    "<div class='alert alert-block alert-success'>\n",
    "Iterate through the HTML tables in the web page to locate the \"Results by riding\" table.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get a list of all the tables in the web page\n",
    "list_of_tables = soup.findAll(\"table\")\n",
    "\n",
    "# Initialize the variable we'll use to store the index\n",
    "result_table_index = None\n",
    "\n",
    "# Iterate over each table in the wikipedia article\n",
    "for ii, table in enumerate(list_of_tables):\n",
    "    first_row = table.findAll('tr')[0].text # Get the first row of the table\n",
    "    if \"Results by riding - 2019 Canadian federal election\" in first_row:\n",
    "        result_table_index = ii  # If we get a match, we've found our table!\n",
    "\n",
    "# Display index of the result table\n",
    "print(result_table_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "Now that we have the index of the table we want, we can easily retrieve it from our `list_of_tables`. All of the juicy, riveting details of the election are almost within reach, but before we can grasp them, we're going to need to know a bit more about how the data is organized. Since this is an HTML table, you can think of it as being built from a large number of 'rows' in the table. Take some time to play around with the table in your browser using development tools. We're interested in figuring out what HTML tag is use to denote a single row in the table (take, for example, Calgary Nose Hill - what's the tag that is used to identify its entire row in the table? Make sure you're inside the table's 'tbody' tag, and not its 'thead' tag).\n",
    "</div>\n",
    "<div class='alert alert-block alert-success'>\n",
    "Find the HTML tag used to designate a single row of data in this table.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the two-letter HTML tag you found as part of your investigation (in string format)\n",
    "row_tag = \"tr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "Now that we have access to the rows of the table, we can look through them to find all of the data corresponding to any given riding! We might as well keep things close to 'home'; in the following code cell, we're going to produce a list of all the rows in the table's body and then locate the row corresponding to the 'Waterloo' riding. \n",
    "</div>\n",
    "<div class='alert alert-block alert-success'>\n",
    "Create a list of all of the rows in the 'Results by riding' table, and then locate the 'Waterloo' riding's row. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Waterloo', 'ON', 'Lib', 'Lib', '31,085', '48.8%', '15,470', '24.3%', '74.8%', '31,085', '15,615', '9,710', '–', '6,184', '1,112', '–', '–', '63,706']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the \"Results by riding\" table from the list of tables\n",
    "results_table = list_of_tables[result_table_index]\n",
    "\n",
    "# Create a list of all rows in the table by searching for the row tag you found\n",
    "all_rows = results_table.findAll(row_tag)\n",
    "\n",
    "# Iterate through list of rows to find 'Waterloo' row.\n",
    "for i, row in enumerate(all_rows):\n",
    "    if \"Waterloo\" in row.text:\n",
    "        waterloo_row = row\n",
    "        \n",
    "# Process the results into a human-readable form:\n",
    "waterloo_text = [r for r in waterloo_row.text.split(\"\\n\") if r]\n",
    "\n",
    "print(waterloo_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "If everything went well, we should now have access to the 'waterloo_text' variable, which is a list of 18 strings we created from the Riding of Waterloo's row in the 'Results by riding' table. Fortunately for us, all of the other data rows should follow the exact same pattern. We can use this inter-row regularity to create a Pandas dataframe that should closely match the table in the wikipedia article. \n",
    "</div>\n",
    "<div class='alert alert-block alert-success'>\n",
    "Remove the first 5 rows of the 'all_rows' variable (index positions 0 to 4). Then, populate the pandas dataframe (we've filled in the column names already) with the rows from the table you scraped. Finally, find the number of ridings from each province and territory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (848668486.py, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\jeffr\\AppData\\Local\\Temp\\ipykernel_3464\\848668486.py\"\u001b[1;36m, line \u001b[1;32m43\u001b[0m\n\u001b[1;33m    riding_counts = riding_df[].value_counts()\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove first 5 items from the 'all_rows' list\n",
    "all_but_five_rows = all_rows[5:]\n",
    "\n",
    "# Initialize a list of the columns from the Wikipedia table\n",
    "riding_cols = [\n",
    "    'riding',\n",
    "    'province_or_territory',\n",
    "    '2015_winning_party',\n",
    "    '2019_winning_party',\n",
    "    'votes',\n",
    "    'share',\n",
    "    'margin_num',\n",
    "    'margin_pct',\n",
    "    'turnout',\n",
    "    'liberal',\n",
    "    'conservative',\n",
    "    'ndp',\n",
    "    'bloc',\n",
    "    'green',\n",
    "    'ppc',\n",
    "    'independent',\n",
    "    'other',\n",
    "    'total',\n",
    "    'riding_url',\n",
    "]\n",
    "\n",
    "# Initialize dataframe\n",
    "riding_df = pd.DataFrame(columns=riding_cols)\n",
    "\n",
    "# Populate dataframe with rows\n",
    "for row in all_but_five_rows:\n",
    "    row_text = [r for r in row.text.replace(',','').split(\"\\n\") if r]\n",
    "    row_text.append(row.find('a', href=True)['href'])\n",
    "\n",
    "    while len(row_text) < 19:\n",
    "        row_text.insert(-2, 0) # This fixes 3 broken rows\n",
    "\n",
    "    df_row = pd.Series(row_text, index=riding_df.columns)\n",
    "    riding_df = riding_df.append(df_row, ignore_index=True)\n",
    "    \n",
    "\n",
    "# Count the number of ridings from each province and territory\n",
    "riding_counts = riding_df['province_or_territory'].value_counts()\n",
    "\n",
    "riding_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "If you're not a student of Canadian politics, it might surprise you to learn that in the 2019 election, the incumbent Liberal Party of Canada (Lib) recieved fewer votes than the Conservative Party of Canada (Con), and yet the Liberals won more seats than the Conservatives and formed government. This happened because each riding in the Canadian electoral system runs according to a winner-takes-all logic, where 100% of the rewards go to the cadidate who finished in first place, even if they only beat the person in second place by a single vote. This means that having a large margin of victory in a single riding is not desirable - presumably, that represents time, resources, and effort that could have been more efficiently allocated. In this question, we're going to see if we can use the data we scraped to help shed some light on why this strange state of affairs came to pass.\n",
    "</div>\n",
    "<div class='alert alert-block alert-success'>\n",
    "Examine the twenty ridings with the highest margin of victory. Then, examine the twenty ridings with the lowest margin of victory. The margin of victory is contained in the 'margin_num' and 'margin_pct' columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the margin_num column to a numeric column\n",
    "riding_df['margin_num'] = pd.to_numeric(riding_df['margin_num'])\n",
    "\n",
    "# Sort the entire dataframe by the value of the margin number, ascending\n",
    "margin_df_ascending  = riding_df.sort_values(['margin_num'], ascending=True)\n",
    "\n",
    "# display the 20 ridings with the largest margin of victory\n",
    "display(HTML('<div class=\"alert alert-block alert-info\">Ridings with highest margin of victory</div>'))\n",
    "display(margin_df_ascending.tail(20))\n",
    "\n",
    "# display the 20 ridings with the lowest margin of victory\n",
    "display(HTML('<div class=\"alert alert-block alert-danger\">Ridings with lowest margin of victory</div>'))\n",
    "display(margin_df_ascending.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "For this final problem, we're going to harness the information we've already gathered to create a scraper that's capable of semi-autonomously traversing a (vanishingly small) proportion of Wikipedia's pages. Specifically, we're going to take advantage of the fact that each of the riding entries that we scraped contains a link to Wikipedia's article on that riding. \n",
    "</div>\n",
    "<div class='alert alert-block alert-success'>\n",
    "Create a scraper capable of retrieving the 'Date created' date for each of the 20 ridings with the lowest margin of victory. Average all of these dates and round them to the nearest full year. Make certain that your scraper is capable of handling riding articles that do not contain a 'First Contested' date.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1975\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize variables\n",
    "link_base = \"https://en.wikipedia.org/\"\n",
    "year_created_list = []\n",
    "\n",
    "# create list of first 20 rows:\n",
    "first_twenty_rows = list(margin_df_ascending.iterrows())[0:20]\n",
    "\n",
    "# retrieve 'District Created' for each link in list\n",
    "for i, row in first_twenty_rows:\n",
    "    sleep(0.5)\n",
    "\n",
    "    r = requests.get(link_base + row['riding_url']) # Send request to Wikipedia\n",
    "    soup = BeautifulSoup(r.content, 'lxml') # Process using beautiful soup\n",
    "    for row in soup.findAll('tr'): # Iterate over our 'soup' DOM\n",
    "        if 'District created' in row.text: # If we find a match, add the value.\n",
    "            year_created_list.append(int(row.find('td').text))\n",
    "            \n",
    "# Find the average year of creation and round it to the nearest full year\n",
    "avg_creation_year = round(sum(year_created_list)/len(year_created_list))\n",
    "\n",
    "print(avg_creation_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.8.5"
   },
   "toc": {
    "base_numbering": 1,
    "nav_menu": {},
    "number_sections": false,
    "sideBar": true,
    "skip_h1_title": false,
    "title_cell": "Table of Contents",
    "title_sidebar": "Contents",
    "toc_cell": false,
    "toc_position": {},
    "toc_section_display": true,
    "toc_window_display": false
   },
   "varInspector": {
    "cols": {
     "lenName": 16,
     "lenType": 16,
     "lenVar": 40
    },
    "kernels_config": {
     "python": {
      "delete_cmd_postfix": "",
      "delete_cmd_prefix": "del ",
      "library": "var_list.py",
      "varRefreshCmd": "print(var_dic_list())"
     },
     "r": {
      "delete_cmd_postfix": ") ",
      "delete_cmd_prefix": "rm(",
      "library": "var_list.r",
      "varRefreshCmd": "cat(var_dic_list()) "
     }
    },
    "types_to_exclude": [
     "module",
     "function",
     "builtin_function_or_method",
     "instance",
     "_Feature"
    ],
    "window_display": false
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
